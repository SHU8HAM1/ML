{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707a62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218dfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip = zipfile.ZipFile('fra-eng.zip')\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e5d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re\n",
    "from unicodedata import normalize\n",
    "from numpy import array,argmax\n",
    "from pickle import load,dump\n",
    "from numpy.random import rand,shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ce5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
    "from nltk.translate.bleu_score import SmoothingFunction,corpus_bleu\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a8ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3b9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def splitting_sentence(doc):\n",
    "\tsentences = doc.strip().split('\\n')\n",
    "\tpairs = [sentence.split('\\t') for sentence in  sentences]\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de8af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pairs(sentences):\n",
    "    cleaned = list()\n",
    " \n",
    "    #preparing regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\n",
    "# preparing translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "  # iterating over each pair\n",
    "    for pair in sentences:\n",
    "        clean_pair = list()\n",
    "  \n",
    "        for sentence in pair:\n",
    "# normalizing unicode characters\n",
    "            sentence = normalize('NFD', sentence).encode('ascii', 'ignore')\n",
    "            sentence = sentence.decode('UTF-8')\n",
    "# tokenizing on white space\n",
    "            sentence = sentence.split()\n",
    "# converting to lowercase\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "# removing punctuation from each token\n",
    "            sentence = [word.translate(table) for word in sentence]\n",
    "# removing non-printable chars form each token\n",
    "            sentence = [re_print.sub('', w) for w in sentence]\n",
    "            \n",
    "            sentence = [word for word in sentence if word.isalpha()]\n",
    "# storing as string\n",
    "            clean_pair.append(' '.join(sentence))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64db457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print(filename,': Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e693a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-french.pkl : Saved\n",
      "English --> French\n",
      "go --> va\n",
      "go --> marche\n",
      "go --> bouge\n",
      "hi --> salut\n",
      "hi --> salut\n",
      "run --> cours\n",
      "run --> courez\n",
      "run --> prenez vos jambes a vos cous\n",
      "run --> file\n",
      "run --> filez\n",
      "run --> cours\n",
      "run --> fuyez\n",
      "run --> fuyons\n",
      "run --> cours\n",
      "run --> courez\n",
      "run --> prenez vos jambes a vos cous\n",
      "run --> file\n",
      "run --> filez\n",
      "run --> cours\n",
      "run --> fuyez\n",
      "run --> fuyons\n",
      "who --> qui\n",
      "wow --> ca alors\n",
      "wow --> waouh\n",
      "wow --> wah\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "filename = 'fra.txt'\n",
    "doc = load_file(filename)\n",
    "\n",
    "# split into english-french pairs\n",
    "pairs = splitting_sentence(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "\n",
    "# save clean pairs to file\n",
    "saving_clean_data(clean_pairs, 'english-french.pkl')\n",
    "\n",
    "print('English','-->',\"French\")\n",
    "# spot check\n",
    "for i in range(25):\n",
    "    print(clean_pairs[i,0],'-->',clean_pairs[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a966b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def loading_cleaned_data(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2250b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194513, 3)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = loading_cleaned_data('english-french.pkl')\n",
    "print(data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59442d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-french-both.pkl : Saved\n",
      "english-french-train.pkl : Saved\n",
      "english-french-test.pkl : Saved\n"
     ]
    }
   ],
   "source": [
    "# reducing dataset size (scaling) \n",
    "\n",
    "new_data_size = 20000\n",
    "dataset = data[:new_data_size, :]\n",
    "\n",
    "# randomly shuffling the dataset to get proper training and testing data\n",
    "shuffle(dataset)\n",
    "\n",
    "# splitting into training and testing (90%-10%)\n",
    "train, test = dataset[:18000], dataset[18000:]\n",
    "\n",
    "# saving the cleaned data,train data and test data \n",
    "saving_clean_data(dataset, 'english-french-both.pkl')\n",
    "saving_clean_data(train, 'english-french-train.pkl')\n",
    "saving_clean_data(test, 'english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac33e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loading_cleaned_data('english-french-both.pkl')\n",
    "train = loading_cleaned_data('english-french-train.pkl')\n",
    "test = loading_cleaned_data('english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c07adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae067709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3414\n",
      "English Max Length: 5\n",
      "French Vocabulary Size: 6988\n",
      "French Max Length: 11\n"
     ]
    }
   ],
   "source": [
    "#english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "#french tokenizer\n",
    "\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "print('French Vocabulary Size: %d' % fra_vocab_size)\n",
    "print('French Max Length: %d' % (fra_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d2d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output sequence must be encoded to integers and padded to the maximum phrase length\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tx = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tx = pad_sequences(x, maxlen=length, padding='post')\n",
    "\treturn x\n",
    "\n",
    "# One hot encoding to max phrase length\n",
    "def one_hot_encoding(sequences, vocab_size):\n",
    "\ty_1 = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\ty_1.append(encoded)\n",
    "\ty = array(y_1)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a09899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training data\n",
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = one_hot_encoding(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare testing data\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer,eng_length, test[:, 0])\n",
    "testY = one_hot_encoding(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c52674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (18000, 11) (18000, 5, 3414)\n",
      "testing size: (2000, 11) (2000, 5, 3414)\n"
     ]
    }
   ],
   "source": [
    "print('training size:',trainX.shape,trainY.shape)\n",
    "print('testing size:',testX.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87bfd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(source_vocab, target_vocab, source_len, target_len, units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(source_vocab, units, input_length=source_len, mask_zero=True))\n",
    "\tmodel.add(LSTM(units))\n",
    "\tmodel.add(RepeatVector(target_len))\n",
    "\tmodel.add(LSTM(units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(target_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef43fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_building(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 512)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8658cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 11, 512)           3577856   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               2099200   \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 5, 512)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 512)            2099200   \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 3414)          1751382   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,527,638\n",
      "Trainable params: 9,527,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66abd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop model if accuracy of the model doesn't changes by more than 0.01 \n",
    "# Patience = 5 : After each 5 epochs if no improvement is there then training will be stopped.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72ddfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "720/720 - 266s - loss: 3.5927 - acc: 0.4664 - val_loss: 3.0713 - val_acc: 0.5190 - 266s/epoch - 369ms/step\n",
      "Epoch 2/20\n",
      "720/720 - 251s - loss: 2.7210 - acc: 0.5537 - val_loss: 2.5952 - val_acc: 0.5803 - 251s/epoch - 348ms/step\n",
      "Epoch 3/20\n",
      "720/720 - 239s - loss: 2.1729 - acc: 0.6111 - val_loss: 2.2558 - val_acc: 0.6193 - 239s/epoch - 331ms/step\n",
      "Epoch 4/20\n",
      "720/720 - 242s - loss: 1.7368 - acc: 0.6594 - val_loss: 2.0429 - val_acc: 0.6453 - 242s/epoch - 336ms/step\n",
      "Epoch 5/20\n",
      "720/720 - 235s - loss: 1.3636 - acc: 0.7077 - val_loss: 1.8961 - val_acc: 0.6663 - 235s/epoch - 327ms/step\n",
      "Epoch 6/20\n",
      "720/720 - 235s - loss: 1.0453 - acc: 0.7588 - val_loss: 1.7852 - val_acc: 0.6912 - 235s/epoch - 327ms/step\n",
      "Epoch 7/20\n",
      "720/720 - 236s - loss: 0.7888 - acc: 0.8063 - val_loss: 1.7221 - val_acc: 0.7074 - 236s/epoch - 327ms/step\n",
      "Epoch 8/20\n",
      "720/720 - 236s - loss: 0.5916 - acc: 0.8479 - val_loss: 1.6956 - val_acc: 0.7148 - 236s/epoch - 327ms/step\n",
      "Epoch 9/20\n",
      "720/720 - 266s - loss: 0.4500 - acc: 0.8809 - val_loss: 1.6811 - val_acc: 0.7215 - 266s/epoch - 370ms/step\n",
      "Epoch 10/20\n",
      "720/720 - 266s - loss: 0.3504 - acc: 0.9036 - val_loss: 1.6731 - val_acc: 0.7311 - 266s/epoch - 369ms/step\n",
      "Epoch 11/20\n",
      "720/720 - 266s - loss: 0.2848 - acc: 0.9197 - val_loss: 1.7014 - val_acc: 0.7292 - 266s/epoch - 370ms/step\n",
      "Epoch 12/20\n",
      "720/720 - 265s - loss: 0.2451 - acc: 0.9281 - val_loss: 1.7295 - val_acc: 0.7313 - 265s/epoch - 369ms/step\n",
      "Epoch 13/20\n",
      "720/720 - 266s - loss: 0.2196 - acc: 0.9332 - val_loss: 1.7452 - val_acc: 0.7317 - 266s/epoch - 369ms/step\n",
      "Epoch 14/20\n",
      "720/720 - 267s - loss: 0.2038 - acc: 0.9355 - val_loss: 1.7447 - val_acc: 0.7360 - 267s/epoch - 370ms/step\n",
      "Epoch 15/20\n",
      "720/720 - 266s - loss: 0.1903 - acc: 0.9384 - val_loss: 1.7946 - val_acc: 0.7319 - 266s/epoch - 370ms/step\n",
      "Epoch 16/20\n",
      "720/720 - 262s - loss: 0.1823 - acc: 0.9397 - val_loss: 1.7944 - val_acc: 0.7333 - 262s/epoch - 364ms/step\n",
      "Epoch 17/20\n",
      "720/720 - 232s - loss: 0.1752 - acc: 0.9409 - val_loss: 1.8113 - val_acc: 0.7362 - 232s/epoch - 322ms/step\n",
      "Epoch 18/20\n",
      "720/720 - 233s - loss: 0.1721 - acc: 0.9401 - val_loss: 1.8243 - val_acc: 0.7348 - 233s/epoch - 323ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1838fb5aa00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(trainX, trainY, epochs= 50, batch_size=25, validation_data=(testX, testY), verbose=2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77985bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:   \n",
    "\t\t\treturn word\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9940106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "  \n",
    "  # Creating empty lists for actual phrases(French) and predicted phrases(English) \n",
    "  actual,predicted = list(),list()\n",
    "  a,b,c = list(),list(),list()\n",
    "  for i,source in enumerate(sources):\n",
    "\n",
    "    # reshaping to the required size\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "\n",
    "    # predicting for the english tokenizer\n",
    "    translation = predict_sequence(model, eng_tokenizer, source)\n",
    "    # raw_dataset = raw_dataset[i].split(' ') \n",
    "    # print(raw_dataset[i][1])\n",
    "\n",
    "    raw_src,raw_target = raw_dataset[i][1],raw_dataset[i][0]\n",
    "    \n",
    "    # First 10 Predictions\n",
    "    if i <= 10:\n",
    "      print('source = ',raw_src,'<--->', ' target = ',raw_target,'<--->','  predicted = ',translation)\n",
    "\n",
    "    actual.append([raw_target.split()])\n",
    "    predicted.append(translation.split())\n",
    "  \n",
    "  # calculating BLEU score\n",
    "  print('-------------------------------------------')\n",
    "  print('BLEU Score :')\n",
    "  print('BLEU score-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "  print('BLEU score-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "  print('BLEU score-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "  print('BLEU score-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie,auto_reweigh=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fde9373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  tom etait sincere <--->  target =  tom was sincere <--->   predicted =  tom meant sincere\n",
      "source =  tom etait tres motive <--->  target =  tom was excited <--->   predicted =  tom was excited\n",
      "source =  ca se passera bien pour vous <--->  target =  youll be fine <--->   predicted =  youll be fine\n",
      "source =  essaie de resister <--->  target =  try to resist <--->   predicted =  try to resist\n",
      "source =  depechetoi de venir <--->  target =  come quickly <--->   predicted =  come quickly\n",
      "source =  tu es un amour <--->  target =  youre a doll <--->   predicted =  youre a doll\n",
      "source =  ils vous ont vu <--->  target =  they saw you <--->   predicted =  they saw you\n",
      "source =  quittez tom <--->  target =  leave tom <--->   predicted =  leave tom\n",
      "source =  il me faut de la nourriture <--->  target =  i need food <--->   predicted =  i need food\n",
      "source =  etesvous catholique <--->  target =  are you catholic <--->   predicted =  are you catholic\n",
      "source =  je ne ronfle pas <--->  target =  i dont snore <--->   predicted =  i dont snore\n",
      "-------------------------------------------\n",
      "BLEU Score :\n",
      "BLEU score-1: 0.936651\n",
      "BLEU score-2: 0.917988\n",
      "BLEU score-3: 0.856543\n",
      "BLEU score-4: 0.602941\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,eng_tokenizer,trainX,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74db1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  il a abandonne tout espoir <--->  target =  he gave up hope <--->   predicted =  he gave up hope\n",
      "source =  vous captez <--->  target =  do you get it <--->   predicted =  you you it\n",
      "source =  estce quelle a aime <--->  target =  did she like it <--->   predicted =  is that for it\n",
      "source =  reste assise <--->  target =  stay seated <--->   predicted =  sit tight\n",
      "source =  ce nest pas un poisson <--->  target =  it isnt a fish <--->   predicted =  its not a fish\n",
      "source =  je vous ai suivie <--->  target =  i followed you <--->   predicted =  i followed you\n",
      "source =  vous etes intrepide <--->  target =  youre fearless <--->   predicted =  youre fearless\n",
      "source =  dirigezvous vers lest <--->  target =  head east <--->   predicted =  head east\n",
      "source =  cest ici que nous sommes <--->  target =  here we are <--->   predicted =  its wide here\n",
      "source =  tu es mechante <--->  target =  youre mean <--->   predicted =  youre wicked\n",
      "source =  jai gere <--->  target =  i handled it <--->   predicted =  i messed\n",
      "-------------------------------------------\n",
      "BLEU Score :\n",
      "BLEU score-1: 0.621895\n",
      "BLEU score-2: 0.519566\n",
      "BLEU score-3: 0.452334\n",
      "BLEU score-4: 0.259334\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
